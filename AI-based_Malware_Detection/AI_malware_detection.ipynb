{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = '../데이터'\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')\n",
        "# data_path = '/content'\n",
        "\n",
        "# !unzip /content/drive/MyDrive/AI-based_Malware_Detection/데이터.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdRPzSdYSAbf",
        "outputId": "7ac7165e-d9fd-4211-8c2a-d44e2aff2ece"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import pprint\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import RFE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bNVKxXs7SAbk"
      },
      "outputs": [],
      "source": [
        "SEED = 41\n",
        "\n",
        "# 실제 데이터 중 정답파일에 대해 딕셔너리 형태로 만들어서 \n",
        "# key를 파일이름으로 만들어주고, 값으로 라벨의 값이 저장되는 딕셔너리 생성\n",
        "def read_label_csv(path):\n",
        "    label_table = dict()\n",
        "    with open(path, \"r\",encoding='ISO-8859-1') as f:\n",
        "        for line in f.readlines()[1:]:\n",
        "            fname, label = line.strip().split(\",\")\n",
        "            label_table[fname] = int(label)\n",
        "    return label_table\n",
        "\n",
        "# json파일 불러오는 함수\n",
        "def read_json(path): \n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# 우리가 사용하게 될 머신런닝들을 정의하는 함수 -> 밑에 \"학습 및 검증\" 부분이다\n",
        "def load_model(**kwargs):\n",
        "    if kwargs[\"model\"] == \"rf\":\n",
        "        return RandomForestClassifier(random_state=kwargs[\"random_state\"], n_jobs=4)\n",
        "    elif kwargs[\"model\"] == \"dt\":\n",
        "        return DecisionTreeClassifier(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"lgb\":\n",
        "        return LGBMClassifier(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"svm\":\n",
        "        return SVC(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"lr\":\n",
        "        return LogisticRegression(random_state=kwargs[\"random_state\"], n_jobs=-1)\n",
        "    elif kwargs[\"model\"] == \"knn\":\n",
        "        return KNeighborsClassifier(n_jobs=-1)\n",
        "    elif kwargs[\"model\"] == \"adaboost\":\n",
        "        return AdaBoostClassifier(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"mlp\":\n",
        "        return MLPClassifier(random_state=kwargs[\"random_state\"])\n",
        "    else:\n",
        "        print(\"Unsupported Algorithm\")\n",
        "        return None\n",
        "    \n",
        "# 모델을 정의한것으로 학습데이터로 학습 시키는 함수\n",
        "def train(X_train, y_train, model): \n",
        "    '''\n",
        "        머신러닝 모델을 선택하여 학습을 진행하는 함수\n",
        "\t\n",
        "        :param X_train: 학습할 2차원 리스트 특징벡터\n",
        "        :param y_train: 학습할 1차원 리스트 레이블 벡터\n",
        "        :param model: 문자열, 선택할 머신러닝 알고리즘\n",
        "        :return: 학습된 머신러닝 모델 객체\n",
        "    '''\n",
        "    clf = load_model(model=model, random_state=SEED)\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "# 정확도를 선출하는 함수\n",
        "def evaluate(X_test, y_test, model): \n",
        "    '''\n",
        "        학습된 머신러닝 모델로 검증 데이터를 검증하는 함수\n",
        "\t\n",
        "        :param X_test: 검증할 2차원 리스트 특징 벡터\n",
        "        :param y_test: 검증할 1차원 리스트 레이블 벡터\n",
        "        :param model: 학습된 머신러닝 모델 객체\n",
        "    '''\n",
        "    predict = model.predict(X_test)\n",
        "    print(model, \"정확도\", model.score(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 레이블 테이블 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_table = read_label_csv(data_path+\"/학습데이터_정답.csv\")\n",
        "label_table2 = read_label_csv(data_path+\"/검증데이터_정답.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpemrbrwSAbq"
      },
      "source": [
        "## 특징 벡터 생성 예시\n",
        "- PEMINER 정보는 모두 수치형 데이터이므로 특별히 가공을 하지 않고 사용 가능\n",
        "- EMBER, PESTUDIO 정보는 가공해서 사용해야 할 특징들이 있음 (e.g. imports, exports 등의 문자열 정보를 가지는 데이터)\n",
        "- 수치형 데이터가 아닌 데이터(범주형 데이터)를 어떻게 가공할 지가 관건 >> 인코딩 (e.g. 원핫인코딩, 레이블인코딩 등)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PEMINER 전체 데이터 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PeminerParser: #모든 데이터\n",
        "    def __init__(self, path):\n",
        "        self.report = read_json(path)\n",
        "        self.vector = []\n",
        "    \n",
        "    def process_report(self):\n",
        "        '''\n",
        "            전체 데이터 사용        \n",
        "        '''\n",
        "        self.vector = [value for _, value in sorted(self.report.items(), key=lambda x: x[0])]\n",
        "        return self.vector\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EMBER 특징 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p3rAFyogSAbz"
      },
      "outputs": [],
      "source": [
        "# 미리 사용된 특징들 같은경우 \n",
        "class EmberParser:\n",
        "    '''\n",
        "        예제에서 사용하지 않은 특징도 사용하여 벡터화 할 것을 권장\n",
        "    '''\n",
        "    def __init__(self, path):\n",
        "        self.report = read_json(path)\n",
        "        self.vector = []\n",
        "    \n",
        "    # histogram 특징 추출\n",
        "    def get_histogram_info(self):\n",
        "        histogram = np.array(self.report[\"histogram\"])\n",
        "        total = histogram.sum()\n",
        "        vector = histogram / total # 평균값으로 특징 가공\n",
        "        return vector.tolist()\n",
        "    \n",
        "    # strings 특징 추출\n",
        "    # 문자열 수, 평균 길이, 문자 histogram, URLS 등과 같은 다양한 패턴과 일치하는 문자열의 수\n",
        "    def get_string_info(self):\n",
        "        strings = self.report[\"strings\"]\n",
        "\n",
        "        # printables의 크기가 0이상이면 실수형태로 저장\n",
        "        hist_divisor = float(strings['printables']) if strings['printables'] > 0 else 1.0\n",
        "        vector = [\n",
        "            strings['numstrings'], \n",
        "            strings['avlength'], \n",
        "            strings['printables'],\n",
        "            strings['entropy'], \n",
        "            # strings['paths'], \n",
        "            # strings['urls'],\n",
        "            strings['registry'], \n",
        "            strings['MZ']\n",
        "        ]\n",
        "        # printabledist의 평균값을 리스트로 저장\n",
        "        vector += (np.asarray(strings['printabledist']) / hist_divisor).tolist()\n",
        "        return vector\n",
        "    \n",
        "    # general 특징 추출\n",
        "    # import, export, symbol의 수와 파일의 relocation, resources, signature 등이 있는지 여부에 대한 수 \n",
        "    def get_general_file_info(self):\n",
        "        general = self.report[\"general\"]\n",
        "        vector = [\n",
        "            general['size'], \n",
        "            general['vsize'], \n",
        "            general['has_debug'], \n",
        "            general['exports'], \n",
        "            general['imports'],\n",
        "            general['has_relocations'], \n",
        "            general['has_resources'], \n",
        "            general['has_signature'], \n",
        "            general['has_tls'],\n",
        "            general['symbols']\n",
        "        ]\n",
        "        return vector\n",
        "\n",
        "    # 특징 추가\n",
        "    def get_histogram_max(self):\n",
        "        histogram = np.array(self.report[\"histogram\"])\n",
        "        vector = [np.max(histogram)] # 최대값으로 특징 가공\n",
        "        return vector\n",
        "\n",
        "    def get_byteentropy_info(self):\n",
        "        byteentropy = np.array(self.report[\"byteentropy\"])\n",
        "        total = byteentropy.sum()\n",
        "        vector = byteentropy / total\n",
        "        return vector.tolist()\n",
        "        \n",
        "    def get_byteentropy_max(self):\n",
        "        byteentropy = np.array(self.report[\"byteentropy\"])\n",
        "        vector = [np.max(byteentropy)]\n",
        "        return vector\n",
        "\n",
        "    def get_byteennum(self):\n",
        "        byteentropy = np.array(self.report[\"byteentropy\"])\n",
        "        vector = [len(byteentropy)]\n",
        "        return vector\n",
        "\n",
        "    #byteentropy 특징 벡터 추가\n",
        "    def get_byteentropy(self): \n",
        "        byteentropy = np.array(self.report[\"byteentropy\"])\n",
        "        vector = byteentropy\n",
        "        return vector.tolist()\n",
        "\n",
        "    def get_string_max(self):\n",
        "        strings = np.array(self.report[\"strings\"]['printabledist'])\n",
        "        vector = [np.max(strings)]\n",
        "        return vector\n",
        "\n",
        "    # 파일에 컴파일된 시스템에 대한 세부정보이다.\n",
        "    # 링커, 이미지, 운영체제 버전 등등을 제공한다.\n",
        "    def get_header_info(self): #header 추가\n",
        "        header = self.report['header']\n",
        "        vector = [\n",
        "            header['coff']['timestamp'],\n",
        "            header['coff']['machine'],\n",
        "            header['coff']['characteristics'],\n",
        "            header['optional']['subsystem'],\n",
        "            header['optional']['dll_characteristics'],\n",
        "            header['optional']['magic'],\n",
        "            header['optional']['major_image_version'],\n",
        "            header['optional']['minor_image_version'],\n",
        "            header['optional']['major_linker_version'],\n",
        "            header['optional']['minor_linker_version'],\n",
        "            header['optional']['major_operating_system_version'],\n",
        "            header['optional']['minor_operating_system_version'],\n",
        "            header['optional']['major_subsystem_version'],\n",
        "            header['optional']['minor_subsystem_version'],\n",
        "            header['optional']['sizeof_code'],\n",
        "            header['optional']['sizeof_headers'],\n",
        "            header['optional']['sizeof_heap_commit'],\n",
        "        ]\n",
        "        return vector\n",
        "\n",
        "    def get_sizeof_code(self):\n",
        "        header = np.array(self.report[\"header\"][\"optional\"][\"sizeof_code\"])\n",
        "        vector = [float(header)]\n",
        "        return vector\n",
        "\n",
        "    # section에 대한 정보들 \n",
        "    # 섹션의 이름, 크기, 엔트로피 및 각 섹션에 대해 주어진 기타 정보를 가진 모든 섹션의 목록\n",
        "    def get_section_number(self):\n",
        "        section = self.report[\"section\"]\n",
        "        vector = [len(section)]\n",
        "        return vector\n",
        "\n",
        "    def get_sectionsize_min(self):\n",
        "        section = np.array(self.report[\"section\"][\"sections\"])\n",
        "        size=[999999999]\n",
        "        for sections in section:\n",
        "            size += sections[\"size\"]\n",
        "        vector = [min(size)]\n",
        "        return vector\n",
        "\n",
        "    def get_section_size(self):\n",
        "        section = self.report[\"section\"]['sections']\n",
        "        size = [-1]\n",
        "        for sections in section:\n",
        "            size += [sections[\"size\"]]\n",
        "        vector = [max(size)]\n",
        "        return vector\n",
        "        \n",
        "    def get_sectionvsize_max(self):\n",
        "        section = np.array(self.report[\"section\"][\"sections\"])\n",
        "        vsize=[999999999]\n",
        "        for sections in section:\n",
        "            vsize += [sections[\"vsize\"]]\n",
        "        vector = [min(vsize)]\n",
        "        return vector\n",
        "\n",
        "    def get_entropy_max(self):\n",
        "        section = np.array(self.report[\"section\"][\"sections\"])\n",
        "        entropy=[999999999]\n",
        "        for sections in section:\n",
        "            entropy += [sections[\"entropy\"]]\n",
        "        vector = [max(entropy)]\n",
        "        return vector\n",
        "\n",
        "    def get_bigentropy_len(self):\n",
        "        section = np.array(self.report[\"section\"][\"sections\"])\n",
        "        cnt = 0\n",
        "        for sections in section:\n",
        "            if sections[\"entropy\"] >= 6:\n",
        "                cnt += 1\n",
        "        vector = [cnt]\n",
        "        return vector\n",
        "\n",
        "    def get_section_info(self):\n",
        "        section = self.report['section']['sections']\n",
        "        vector = []\n",
        "        for sections in section:\n",
        "            vector += [\n",
        "                sections['size'],\n",
        "                sections['entropy'],\n",
        "                sections['vsize'],\n",
        "            ]\n",
        "        return vector\n",
        "\n",
        "    def numimport(self):\n",
        "        imports = self.report[\"imports\"]\n",
        "        vector = [len(imports)]\n",
        "        return vector\n",
        "\n",
        "    def numexport(self):\n",
        "        exports = self.report[\"exports\"]\n",
        "        vector = [len(exports)]\n",
        "        return vector\n",
        "\n",
        "    def get_datadirnum(self):\n",
        "        datadirectories = np.array(self.report[\"datadirectories\"])\n",
        "        vector = [len(datadirectories)]\n",
        "        return vector\n",
        "\n",
        "    def get_datadirsize_max(self):\n",
        "        datadirectories = np.array(self.report[\"datadirectories\"])\n",
        "        size=[-1]\n",
        "        for data in datadirectories:\n",
        "            size.append(data[\"size\"])\n",
        "        vector = [max(size)]\n",
        "        return vector\n",
        "\n",
        "    # datadirectories size 특징 추가\n",
        "    def get_datadirectories(self): \n",
        "        datadirectories = self.report[\"datadirectories\"]\n",
        "        vector = []\n",
        "        for data in datadirectories:\n",
        "            # vector += data[\"size\"], data[\"virtual_address\"]\n",
        "            vector += [data[\"size\"] if data[\"size\"]>100 else 0]\n",
        "            vector += [data[\"virtual_address\"] if data[\"virtual_address\"]>100000 else 0]\n",
        "        return vector\n",
        "\n",
        "\n",
        "    def process_report(self):\n",
        "        vector = []\n",
        "        # 선택하지 않은 feature\n",
        "        # vector += self.get_histogram_info()\n",
        "        # vector += self.get_string_info()\n",
        "        # vector += self.get_byteentropy_info()\n",
        "        # vector += self.get_byteentropy_max()\n",
        "        # vector += self.get_sizeof_code()\n",
        "        # vector += self.get_section_size()\n",
        "        # vector += self.get_sectionvsize_max()\n",
        "        # vector += self.numimport()       \n",
        "        # vector += self.numexport()\n",
        "        # vector += self.get_datadirsize_max()\n",
        "\n",
        "        # 최종 선택 feature\n",
        "        vector += self.get_general_file_info()\n",
        "        vector += self.get_histogram_max() \n",
        "        vector += self.get_byteennum()\n",
        "        vector += self.get_byteentropy() \n",
        "        vector += self.get_string_max()   \n",
        "        vector += self.get_section_number()\n",
        "        vector += self.get_entropy_max() \n",
        "        vector += self.get_bigentropy_len()\n",
        "        vector += self.get_datadirnum()\n",
        "\n",
        "        return vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PESTUDIO 특징 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PestudioParser:\n",
        "    def __init__(self, path):\n",
        "        try:\n",
        "          self.report = read_json(path)\n",
        "        except:\n",
        "          self.report = None\n",
        "        self.vector = []\n",
        "    '''\n",
        "        사용할 특징을 선택하여 벡터화 할 것을 권장\n",
        "    '''\n",
        "    def overview_entropy(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"][\"overview\"][\"entropy\"]\n",
        "            vector = [float(image)]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    def get_indicators_info(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"][\"indicators\"][\"indicator\"]\n",
        "            cnt = 0\n",
        "            for images in image:\n",
        "                if(images[\"@severity\"] == \"1\" or images[\"@severity\"] == \"2\"):\n",
        "                    cnt += 1\n",
        "            vector = [cnt]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    #indicators 필드의 severity 검사. 1인경우 위험도가 가장 높음\n",
        "    #severity의 평균적인 값을 구함\n",
        "    def get_indicators_ave(self):\n",
        "        try:\n",
        "            image = self.report['image']['indicators']['indicator']\n",
        "            avg = 0\n",
        "            for images in image:\n",
        "                avg += int(images['@severity'])\n",
        "            vector = [avg/len(image)]\n",
        "        except:\n",
        "            vector = [0]\n",
        "        return vector\n",
        "\n",
        "\n",
        "    def numsection(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"][\"sections\"][\"section\"]\n",
        "            vector = [(len(image))]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    def blacklist(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"][\"sections\"][\"section\"]\n",
        "            cnt = 0\n",
        "            for images in image:\n",
        "                if images[\"@blacklisted\"] == \"x\":\n",
        "                    cnt += 1\n",
        "            vector = [cnt]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    def libraries_blacklist(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"][\"libraries\"][\"library\"]\n",
        "            cnt = 0\n",
        "            for images in image:\n",
        "                if images[\"@blacklist\"] == \"x\":\n",
        "                    cnt += 1\n",
        "            vector = [cnt]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    # 악성코드에서 자주 사용하는 API에 대한 blacklist를 제공하고, 그 리스트에 따라 API가 분류된다.\n",
        "    def import_blacklist(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"][\"imports\"][\"import\"]\n",
        "            cnt = 0\n",
        "            for images in image:\n",
        "                if images[\"@blacklist\"] == \"x\":\n",
        "                    cnt += 1\n",
        "            vector = [cnt]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    def size(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"][\"resources\"][\"instance\"]\n",
        "            size = []\n",
        "            for images in image:\n",
        "                size.append(int(images['@size']))\n",
        "            vector = [max(size)]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector     \n",
        "\n",
        "    def entropy(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"][\"resources\"][\"instance\"]\n",
        "            entropy = []\n",
        "            for images in image:\n",
        "                entropy.append(int(images['@entropy']))\n",
        "            vector = [max(entropy)]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector   \n",
        "\n",
        "    def get_debug(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"][\"debug\"]  \n",
        "            vector = [0 if image == 'n/a' else 1]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    def get_string_leng(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"]['strings']['ascii']['string']\n",
        "            vector = [len(image)]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    def string_size_max(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"][\"strings\"][\"string\"]\n",
        "            size = [-1]\n",
        "            for images in image:\n",
        "                size.append(int(images['@size']))\n",
        "            vector = [max(size)]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    # tls callback을 이용한 악성코드가 존재한다\n",
        "    # 악성 코드를 파일의 이 부분에 저장하여 애플리케이션이 프로세스를 시작하기 전에 악용하는 경우가 있음\n",
        "    def get_tls_callbacks(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"][\"tls-callbacks\"]\n",
        "            vector = [0 if image == 'n/a' else 1]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    # 인증서가 있다면 신뢰할 수 있기에 검사함\n",
        "    def get_certificate(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"]['certificate']\n",
        "            vector = [0 if image == 'n/a' else 1]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector\n",
        "\n",
        "    def get_overlay(self):\n",
        "        try:\n",
        "            image = self.report[\"image\"][\"overlay\"]\n",
        "            vector = [0 if image == 'n/a' else 1]\n",
        "        except:\n",
        "            vector = [-1]\n",
        "        return vector   \n",
        "\n",
        "    # checksum이 0인지 여부를 체크함\n",
        "    def checksum_compare(self):\n",
        "        try:\n",
        "            image = self.report['image']['optional-header']['file-checksum']\n",
        "            if image == '0x00000000':\n",
        "                vector = [0]\n",
        "            else:\n",
        "                vector = [2]\n",
        "        except:\n",
        "            vector = [1]\n",
        "        return vector\n",
        "\n",
        "    # section 필드를 검사함\n",
        "    # txt 쓰기 권한이 있는 경우 악성 코드일 가능성이 있다.\n",
        "    def get_section(self):\n",
        "        try:\n",
        "            section = self.report['image']['sections']['section'][0]\n",
        "            if section['@writable'] == \"x\": \n",
        "                vector = [1]\n",
        "            else:\n",
        "                vector = [0]\n",
        "        except:\n",
        "            vector = [0]\n",
        "        return vector\n",
        "\n",
        "    \n",
        "\n",
        "    # string 필드에 network가 많이 나오면 악성코드 가능성 증가\n",
        "    def get_string_network(self):\n",
        "        try:\n",
        "            image = self.report['image']['strings']['ascii']['string']\n",
        "            cnt = 0\n",
        "            for images in image:\n",
        "                if images['@group'] == 'network':\n",
        "                    cnt+=1\n",
        "            vector = [cnt]\n",
        "        except:\n",
        "            vector = [0]\n",
        "        return vector\n",
        "\n",
        "    def process_report(self):\n",
        "        vector = []\n",
        "        # 선택하지 않은 feature\n",
        "        # vector += self.get_indicators_info()\n",
        "        # vector += self.numsection()\n",
        "        # vector += self.blacklist()\n",
        "        # vector += self.libraries_blacklist()\n",
        "        # vector += self.get_debug()\n",
        "        # vector += self.get_certificate()\n",
        "        # vector += self.checksum_compare()\n",
        "        # vector += self.get_section()\n",
        "\n",
        "        # 최종 선택 feature\n",
        "        vector += self.overview_entropy()\n",
        "        vector += self.import_blacklist()\n",
        "        vector += self.size()\n",
        "        vector += self.entropy()\n",
        "        vector += self.get_string_leng()\n",
        "        vector += self.string_size_max()\n",
        "        vector += self.get_tls_callbacks()\n",
        "        vector += self.get_overlay()\n",
        "        vector += self.get_indicators_ave()\n",
        "        vector += self.get_string_network()\n",
        "        \n",
        "        return vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPb9HsS4SAb2"
      },
      "source": [
        "## 학습데이터 구성\n",
        "- 특징 벡터 구성은 2차원이 되어야함 e.g.  [vector_1, vector_2, ..., vector_n]\n",
        "\n",
        "- 각 벡터는 1차원 리스트, 벡터 크기는 모두 같아야함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1uQfgwJbSAb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20000, 471) (20000,)\n"
          ]
        }
      ],
      "source": [
        "# 데이터의 특징 벡터 모음(2차원 리스트) : X\n",
        "# 데이터의 레이블 모음(1차원 리스트) : y\n",
        "# 실제 데이터를 가지고온다\n",
        "X, y = [], []\n",
        "\n",
        "path_dir = f\"{data_path}/PEMINER/학습데이터\"\n",
        "\n",
        "file_list = os.listdir(path_dir)\n",
        "\n",
        "for i in range(len(file_list)):\n",
        "    feature_vector = []\n",
        "    for data in [\"PEMINER\",\"EMBER\",\"PESTUDIO\"]:\n",
        "        path = f\"{data_path}/{data}/학습데이터/{file_list[i]}\"\n",
        "        label = label_table[file_list[i].split('.')[0]]\n",
        "        if data == \"PEMINER\": \n",
        "            feature_vector += PeminerParser(path).process_report()\n",
        "        elif data == \"EMBER\":\n",
        "            feature_vector += EmberParser(path).process_report()\n",
        "        elif data == \"PESTUDIO\":\n",
        "            feature_vector += PestudioParser(path).process_report()\n",
        "            \n",
        "    X.append(feature_vector)\n",
        "    y.append(label)\n",
        "\n",
        "print(np.asarray(X).shape, np.asarray(y).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 검증데이터 구성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 471) (10000,)\n"
          ]
        }
      ],
      "source": [
        "X_test, y_test = [], []\n",
        "\n",
        "path_dir = f\"{data_path}/PEMINER/검증데이터\"\n",
        "file_list = os.listdir(path_dir)\n",
        "\n",
        "for i in range(len(file_list)):\n",
        "    feature_vector = []\n",
        "    for data in [\"PEMINER\",\"EMBER\",\"PESTUDIO\"]:\n",
        "        path = f\"{data_path}/{data}/검증데이터/{file_list[i]}\"\n",
        "        label = label_table2[file_list[i].split('.')[0]]\n",
        "        if data == \"PEMINER\": \n",
        "            feature_vector += PeminerParser(path).process_report()\n",
        "        elif data == \"EMBER\":\n",
        "            feature_vector += EmberParser(path).process_report()\n",
        "        elif data == \"PESTUDIO\":\n",
        "            feature_vector += PestudioParser(path).process_report()\n",
        "    \n",
        "    X_test.append(feature_vector)\n",
        "    y_test.append(label)\n",
        "    \n",
        "print(np.asarray(X_test).shape, np.asarray(y_test).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiaocTsNSAb7"
      },
      "source": [
        "## 학습 및 검증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MiyPyVGPSAb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForestClassifier(n_jobs=4, random_state=41) 정확도 0.9505\n",
            "LGBMClassifier(random_state=41) 정확도 0.9542\n"
          ]
        }
      ],
      "source": [
        "# 학습 : 피마이너랑 엠버를 학습시켜서 학습된 특징벡터들을 models에 넣는다 \n",
        "models = []\n",
        "for model in [\"rf\", \"lgb\"]:\n",
        "    clf = train(X, y, model)\n",
        "    models.append(clf)\n",
        "\n",
        "# 검증 피마이너랑 엠버를 각각 검증 \n",
        "# 실제 검증 시에는 제공한 검증데이터를 검증에 사용해야 함\n",
        "for model in models:\n",
        "    evaluate(X_test, y_test, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ensemble_result(X, y, models):\n",
        "    '''\n",
        "        학습된 모델들의 결과를 앙상블하는 함수\n",
        "        \n",
        "        :param X: 검증할 2차원 리스트 특징 벡터\n",
        "        :param y: 검증할 1차원 리스트 레이블 벡터\n",
        "        :param models: 1개 이상의 학습된 머신러닝 모델 객체를 가지는 1차원 리스트\n",
        "    '''\n",
        "    predicts = []\n",
        "    for model in models:\n",
        "        prob = [result for _, result in model.predict_proba(X)]\n",
        "        predicts.append(prob)\n",
        "    \n",
        "    predict = np.mean(predicts, axis=0)\n",
        "    predict = [1 if x >= 0.5 else 0 for x in predict]\n",
        "        \n",
        "    print(\"앙상블 후 정확도\", accuracy_score(y, predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "앙상블 후 정확도 0.957\n"
          ]
        }
      ],
      "source": [
        "ensemble_result(X_test, y_test, models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 테스트 데이터 구성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 471)\n"
          ]
        }
      ],
      "source": [
        "test_vector = []\n",
        "test_label = []\n",
        "\n",
        "path_dir = f\"{data_path}/PEMINER/테스트데이터\"\n",
        "\n",
        "file_list = os.listdir(path_dir)\n",
        "\n",
        "for i in range(len(file_list)):\n",
        "    feature_vector = []\n",
        "    for data in [\"PEMINER\",\"EMBER\",\"PESTUDIO\"]:\n",
        "        path = f\"{data_path}/{data}/테스트데이터/{file_list[i]}\"\n",
        "        label = file_list[i].split('.')[0]\n",
        "        if data == \"PEMINER\": \n",
        "            feature_vector += PeminerParser(path).process_report()\n",
        "        elif data == \"EMBER\":\n",
        "            feature_vector += EmberParser(path).process_report()\n",
        "        elif data == \"PESTUDIO\":\n",
        "            feature_vector += PestudioParser(path).process_report()\n",
        "    test_vector.append(feature_vector)\n",
        "    test_label.append(label)\n",
        "\n",
        "print(np.asarray(test_vector).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 테스트 데이터 학습 및 앙상블"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ensemble_data(X, models):\n",
        "    '''\n",
        "        학습된 모델들의 결과를 앙상블하는 함수\n",
        "          \n",
        "        :param X: 검증할 2차원 리스트 특징 벡터\n",
        "        :param models: 1개 이상의 학습된 머신러닝 모델 객체를 가지는 1차원 리스트\n",
        "    '''\n",
        "\n",
        "    predicts = []\n",
        "    for model in models:\n",
        "        prob = [result for _, result in model.predict_proba(X)]\n",
        "        predicts.append(prob)\n",
        "    \n",
        "    predict = np.mean(predicts, axis=0)\n",
        "    predict = [ 1 if x >= 0.5 else 0 for x in predict ]\n",
        "        \n",
        "    return predict\n",
        "  \n",
        "result = ensemble_data(test_vector, models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 테스트 데이터 예측 : CSV 파일 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "predict_test = []\n",
        "\n",
        "for i in range(len(test_label)):\n",
        "    predict_test.append([test_label[i], result[i]])\n",
        "\n",
        "path = \"./result.csv\"\n",
        "\n",
        "with open(path, 'w', newline='') as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow(['file', 'predict'])\n",
        "    for i in predict_test :\n",
        "        w.writerow(i)\n",
        "        \n",
        "    f.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ai_malware.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
